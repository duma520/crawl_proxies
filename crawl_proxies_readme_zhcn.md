### 🤖 Assistant

# **`crawl_proxies.py` 使用说明书**

---

## **概述**
`crawl_proxies.py` 是一个用于从多个网站爬取代理 IP 的 Python 脚本。它支持多线程爬取、代理验证、去重、保存结果等功能，适用于需要获取大量代理 IP 的场景。无论是开发者、数据分析师、网络安全工程师，还是普通用户，都可以通过本脚本快速获取可用的代理 IP。

---

## **适用人群**
- **开发者**：需要代理 IP 进行网络爬虫开发或测试。
- **数据分析师**：需要代理 IP 进行数据采集。
- **网络安全工程师**：需要代理 IP 进行安全测试或匿名访问。
- **普通用户**：需要代理 IP 访问受限资源或保护隐私。

---

## **功能特性**
1. **多线程爬取**：支持从多个网站同时爬取代理 IP，提高效率。
2. **代理验证**：支持验证代理 IP 的可用性。
3. **去重功能**：自动去除重复的代理 IP。
4. **保存结果**：将爬取的代理 IP 保存到文件中。
5. **挂机模式**：支持无人值守的挂机模式，定期重启脚本。
6. **自定义参数**：支持通过命令行参数自定义爬取和验证的行为。

---

## **快速开始**

### **1. 环境准备**
- **Python 版本**：3.6 或更高版本。
- **依赖库安装**：
  ```bash
  pip install requests beautifulsoup4
  ```

### **2. 运行脚本**
1. 将脚本保存为 `crawl_proxies.py`。
2. 在终端中运行以下命令：
   ```bash
   python crawl_proxies.py
   ```

---

## **详细使用说明**

### **1. 命令行参数**
以下是脚本支持的所有命令行参数及其说明：

| 参数                        | 说明                                                                 |
|-----------------------------|----------------------------------------------------------------------|
| `--proxy`                   | 指定代理服务器（例如：`http://127.0.0.1:8080` 或 `socks5://127.0.0.1:1080`）。 |
| `--validate`                | 在保存前验证代理 IP 是否可用。                                        |
| `--show`                    | 实时显示爬取到的代理 IP。                                             |
| `--verify-url`              | 指定用于验证代理的目标网站（默认：`https://www.google.com`）。         |
| `--verify-ssl`              | 启用 SSL 验证（默认禁用）。                                           |
| `--add-prefix`              | 保存代理 IP 时添加前缀（`http://`、`https://` 或 `socks5://`）。      |
| `--timestamp`               | 保存代理 IP 时在文件名中添加时间戳（默认保存到 `proxy_list.txt`）。    |
| `--deduplicate`             | 对文件中的代理 IP 进行去重。                                          |
| `--deduplicate-file`        | 指定需要去重的文件（默认：`proxy_list.txt`）。                        |
| `--deduplicate-after-save`  | 保存代理 IP 后对文件内容进行去重。                                    |
| `--show-invalid`            | 显示无效代理的警告信息。                                              |
| `--simple-report`           | 显示简单的代理获取报告。                                              |
| `--verbose`                 | 显示详细的调试信息。                                                  |
| `--overnight`               | 启用通宵挂机无人值守模式。                                            |
| `--interval`                | 自定义每次爬取的间隔时间（单位为秒，默认：2 秒）。                    |
| `--validate-timeout`        | 验证代理 IP 的超时时间（单位为秒，默认：10 秒）。                     |
| `--request-timeout`         | 发送请求的超时时间（单位为秒，默认：30 秒）。                         |

---

### **2. 配置文件**
- **代理网站列表**：将需要爬取的代理网站 URL 保存到 `proxy_sites.txt` 文件中，每行一个 URL。
  ```
  http://example.com/proxy
  http://another-example.com/proxy
  ```

---

### **3. 示例命令**

#### **基本用法**
```bash
python crawl_proxies.py
```
- 从 `proxy_sites.txt` 中列出的网站爬取代理 IP，并保存到 `proxy_list.txt`。

#### **验证代理 IP**
```bash
python crawl_proxies.py --validate
```
- 爬取代理 IP 并验证其可用性，仅保存有效的代理 IP。

#### **挂机模式**
```bash
python crawl_proxies.py --overnight --interval 60
```
- 启用挂机模式，每隔 60 秒重启一次脚本。

#### **自定义超时时间**
```bash
python crawl_proxies.py --validate-timeout 15 --request-timeout 60
```
- 设置验证代理的超时时间为 15 秒，发送请求的超时时间为 60 秒。

#### **保存带时间戳的文件**
```bash
python crawl_proxies.py --timestamp
```
- 将代理 IP 保存到带时间戳的文件中，例如 `proxy_list_20231025_123456.txt`。

---

## **输出文件**
- **默认输出文件**：`proxy_list.txt`。
- **带时间戳的文件**：`proxy_list_YYYYMMDD_HHMMSS.txt`（启用 `--timestamp` 时生成）。

---

## **常见问题**

### **1. 为什么爬取的代理 IP 无效？**
- 代理 IP 可能已经失效，建议启用 `--validate` 参数验证代理的可用性。
- 目标网站可能限制了爬取频率，可以增加 `--interval` 的值。

### **2. 如何提高爬取效率？**
- 使用多线程爬取（默认已启用）。
- 增加 `--interval` 的值，避免触发目标网站的反爬机制。

### **3. 挂机模式如何退出？**
- 按下 `Ctrl + C` 终止脚本运行。

---

## **技术支持**
如有任何问题或建议，请联系开发者或提交 Issue 到 GitHub 仓库。

---

## **免责声明**
- 本脚本仅供学习和研究使用，请勿用于非法用途。
- 使用代理 IP 访问受限资源时，请遵守相关法律法规。

---

## **更新日志**
- **v12.0**：初始版本，支持基本爬取、验证和保存功能。
- **v13.0**：增加挂机模式和自定义超时时间功能。

---

## **结语**
希望本脚本能帮助你高效获取代理 IP！如果有任何改进建议，欢迎反馈！